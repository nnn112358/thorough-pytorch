# 3.1 考え方：深層学習を完成させる要素

本節のゴール：

- 機械学習/深層学習タスクの全体フロー
- 各パートの役割を把握する

典型的機械学習タスクは次の手順で進みます。まずは前処理（形式の統一、異常値の除去、必要な変換）を行い、学習/検証/テストに分割します（`train_test_split` や KFold など）。次にモデル選択、損失関数と最適化手法、ハイパーパラメータを決め、学習データでフィットさせ、検証/テストで性能を評価します。

深層学習も流れは同様ですが実装は大きく異なります。まずデータ量が大きいため、全件読み込みは困難です。バッチ学習で毎回一定数のサンプルだけを読み出す設計（データローダ）が必要です。

モデル構築も異なります。多層ネットや畳み込み、プーリング、BatchNorm、LSTM など多様な層を組み合わせるため、層ごとに積み上げるか、機能モジュールを定義して組み合わせる“モジュール化”が基本です。

続いて損失関数と最適化器の設定です。考え方は機械学習と同じですが、任意のモデル構造で逆伝播が動くことが重要です。

準備が整えば学習開始です。デフォルトは CPU 実行なので、GPU を使う場合はモデル・データを GPU に移し、損失関数・最適化も GPU で動くようにします。複数 GPU では割り当てと集約、評価時は CPU へ戻すなど、GPU 設定/操作が関わります。

**深層学習の特徴は、バッチ単位でデータを読み込み、GPU で順伝播→損失→逆伝播→最適化を繰り返すことです。各モジュールの連携が重要で、学習/検証後に指標で性能を評価します。**

ここまでが一連の流れです。以降では、PyTorch がこれらをどう支援しているか、フレームワークとしてのモジュール化の利点を見ていきます。
