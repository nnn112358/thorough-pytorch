# 5.1 PyTorch モデル定義の方法

深層学習ではモデルが核となります。CNN は画像・動画、RNN/LSTM は系列、GNN はグラフに強みがあり、研究紹介でも「どのモデルを使ったか」は最初に問われる点です。本章では PyTorch におけるモデル定義を体系的に学びます。

前半の第3章で層の定義と基本的なモデル構築を学びました。ここでは PyTorch のモデル定義法を整理し、柔軟な設計の基礎を固めます。

本節のゴール：

- PyTorch の3つのモデル定義法を把握
- GitHub 上の多様な記述を読み解ける
- 目的に応じて適切な定義法を選べる



## 5.1.1 予備知識の整理

- `nn.Module` は全てのモジュールの基底クラス。継承してモデルを定義する。
- 定義はおおむね `__init__`（層や部品の初期化）と `forward`（データフロー）からなる。

`nn.Module` を基礎に、`Sequential`、`ModuleList`、`ModuleDict` の3方式でモデルを構成できます。

下面我们就来逐个探索这三种模型定义方式。



## 5.1.2 Sequential

対応は `nn.Sequential()` です。前向きが「層を直列に並べるだけ」の場合に簡潔に定義できます。OrderedDict またはモジュール列を受け取り、追加順に順伝播します。自作の `MySequential` 例で理解を深めます：

```python
from collections import OrderedDict
class MySequential(nn.Module):
    def __init__(self, *args):
        super(MySequential, self).__init__()
        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict
            for key, module in args[0].items():
                self.add_module(key, module)  
                # add_module方法会将module添加进self._modules(一个OrderedDict)
        else:  # 传入的是一些Module
            for idx, module in enumerate(args):
                self.add_module(str(idx), module)
    def forward(self, input):
        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成
        for module in self._modules.values():
            input = module(input)
        return input
```

`Sequential` による定義は層を順に並べるだけです。記述方法は2通り：

- そのまま並べる 

```python
import torch.nn as nn
net = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 10), 
        )
print(net)
```

```
Sequential(
  (0): Linear(in_features=784, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)
```

- OrderedDict を使う：

```python
import collections
import torch.nn as nn
net2 = nn.Sequential(collections.OrderedDict([
          ('fc1', nn.Linear(784, 256)),
          ('relu1', nn.ReLU()),
          ('fc2', nn.Linear(256, 10))
          ]))
print(net2)
```

```
Sequential(
  (fc1): Linear(in_features=784, out_features=256, bias=True)
  (relu1): ReLU()
  (fc2): Linear(in_features=256, out_features=10, bias=True)
)
```

`Sequential` は簡潔・可読で `forward` も不要ですが、中間で外部入力を混ぜたいような柔軟な分岐/結合には不向きです。用途で使い分けます。



## 5.1.3 ModuleList

対応は `nn.ModuleList()`。`nn.Module` のリストを保持し、`append`/`extend` などリスト操作ができます。保持したモジュールはネットワークの一部としてパラメータ登録されます。

```python
net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])
net.append(nn.Linear(256, 10)) # # 类似List的append操作
print(net[-1])  # 类似List的索引访问
print(net)
```

```
Linear(in_features=256, out_features=10, bias=True)
ModuleList(
  (0): Linear(in_features=784, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)
```

要特别注意的是，`nn.ModuleList` 并没有定义一个网络，它只是将不同的模块储存在一起。`ModuleList`中元素的先后顺序并不代表其在网络中的真实位置顺序，需要经过forward函数指定各个层的先后顺序后才算完成了模型的定义。具体实现时用for循环即可完成：

```python
class model(nn.Module):
  def __init__(self, ...):
    super().__init__()
    self.modulelist = ...
    ...
    
  def forward(self, x):
    for layer in self.modulelist:
      x = layer(x)
    return x
```



## 5.1.4 ModuleDict

对应模块为`nn.ModuleDict()`。

`ModuleDict`和`ModuleList`的作用类似，只是`ModuleDict`能够更方便地为神经网络的层添加名称。

```python
net = nn.ModuleDict({
    'linear': nn.Linear(784, 256),
    'act': nn.ReLU(),
})
net['output'] = nn.Linear(256, 10) # 添加
print(net['linear']) # 访问
print(net.output)
print(net)
```

```
Linear(in_features=784, out_features=256, bias=True)
Linear(in_features=256, out_features=10, bias=True)
ModuleDict(
  (act): ReLU()
  (linear): Linear(in_features=784, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
```



## 5.1.5 三种方法的比较与适用场景

`Sequential`适用于快速验证结果，因为已经明确了要用哪些层，直接写一下就好了，不需要同时写`__init__`和`forward`；

ModuleList和ModuleDict在某个完全相同的层需要重复出现多次时，非常方便实现，可以”一行顶多行“；

当我们需要之前层的信息的时候，比如 ResNets 中的残差计算，当前层的结果需要和之前层中的结果进行融合，一般使用 ModuleList/ModuleDict 比较方便。



## 本节参考

【1】https://zhuanlan.zhihu.com/p/64990232

