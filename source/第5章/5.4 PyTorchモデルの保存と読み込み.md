# 5.4 PyTorch モデルの保存と読み込み

これまでに PyTorch モデルの構築と変更を見てきました。ここでは学習済みモデル/重みの保存・読み込みを整理します。

また多 GPU 学習では、保存/読み込みが単 GPU の場合とどう異なるかも確認します（2.3 のデータ並列前提）。

本節のゴール：

- モデルの保存形式
- モデル/重みの保存方法
- 単一/複数 GPU での保存/読み込み
- オプティマイザの状態保存


## 5.4.1 保存形式

拡張子は `.pkl`/`.pt`/`.pth` などが一般的で、使い分けに大差はありません。


## 5.4.2 保存内容

PyTorch モデルは「構造（クラス定義）」と「重み（state_dict）」で構成されます。保存は「構造ごと保存」と「state_dict のみ保存」の2通りがあります。

```python
from torchvision import models
model = models.resnet152(pretrained=True)
save_dir = './resnet152.pth'

# 保存整个模型
torch.save(model, save_dir)
# 保存模型权重
torch.save(model.state_dict, save_dir)
```

いずれの拡張子でも構造/重みの保存に使えます。



## 5.4.3 単一/複数 GPU の違い

GPU への配置は `.cuda()` と `.to(device)` の2通り（ここでは前者で説明）。多 GPU では `torch.nn.DataParallel` を使います：

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 如果是多卡改成类似0,1,2
model = model.cuda()  # 单卡
model = torch.nn.DataParallel(model).cuda()  # 多卡
```

多 GPU の `DataParallel` モデルは各層名の先頭に `module.` が付きます。

- 单卡模型的层名：

![img](https://pic3.zhimg.com/v2-3490f6ab8bc806274dd017e1a66e2486_b.png)

- 多卡模型的层名：

![img](https://pic3.zhimg.com/v2-4b611c24c2e702749cebbe65eaff7cde_b.png)

この差異により保存/読み込み時の取り扱いが変わる点があります。以下で場合分けします。


## 5.4.4 ケース別の扱い

学習/推論の環境差で単/複数 GPU の不一致が生じることがあります。ここでは組合せ（=4）で整理します。例として torchvision の `resnet152` を用います。

- **単一GPUで保存 + 単一GPUで読み込み**

環境変数で使用 GPU を指定して保存/読み込みします。保存時と読み込み時で GPU が異なっても問題ありません。

```python
import os
import torch
from torchvision import models

os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号
model = models.resnet152(pretrained=True)
model.cuda()

save_dir = 'resnet152.pt'   #保存路径

# 保存+读取整个模型
torch.save(model, save_dir)
loaded_model = torch.load(save_dir)
loaded_model.cuda()

# 保存+读取模型权重
torch.save(model.state_dict(), save_dir)
loaded_model = models.resnet152()   #注意这里需要对模型结构有定义
loaded_model.load_state_dict(torch.load(save_dir))
loaded_model.cuda()
```

- **単一GPUで保存 + 複数GPUで読み込み**

単一で保存したモデルを読み込んだ後、`nn.DataParallel` を適用します（`.cuda()` を置き換えるイメージ）。

```python
import os
import torch
from torchvision import models

os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号
model = models.resnet152(pretrained=True)
model.cuda()

# 保存+读取整个模型
torch.save(model, save_dir)

os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #这里替换成希望使用的GPU编号
loaded_model = torch.load(save_dir)
loaded_model = nn.DataParallel(loaded_model).cuda()

# 保存+读取模型权重
torch.save(model.state_dict(), save_dir)

os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #这里替换成希望使用的GPU编号
loaded_model = models.resnet152()   #注意这里需要对模型结构有定义
loaded_model.load_state_dict(torch.load(save_dir))
loaded_model = nn.DataParallel(loaded_model).cuda()
```

- **複数GPUで保存 + 単一GPUで読み込み**

要点は state_dict のキー先頭 `module.` をどう扱うかです。

構造ごと保存した場合は `.module` を取り出します：

```python
import os
import torch
from torchvision import models

os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'   #这里替换成希望使用的GPU编号

model = models.resnet152(pretrained=True)
model = nn.DataParallel(model).cuda()

# 保存+读取整个模型
torch.save(model, save_dir)

os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号
loaded_model = torch.load(save_dir).module
```

state_dict の場合は以下の考え方があります：
**保存時に `model.module.state_dict()` を保存する**
```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #这里替换成希望使用的GPU编号
import torch
from torchvision import models

save_dir = 'resnet152.pth'   #保存路径
model = models.resnet152(pretrained=True)
model = nn.DataParallel(model).cuda()

# 保存权重
torch.save(model.module.state_dict(), save_dir)
```
こうしておけば単一 GPU と同じ形式になり、そのまま読み込めます（推奨）。
**`module.` を外す代わりに読み込み側を `DataParallel` にする**

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #这里替换成希望使用的GPU编号
import torch
from torchvision import models

model = models.resnet152(pretrained=True)
model = nn.DataParallel(model).cuda()

# 保存+读取模型权重
torch.save(model.state_dict(), save_dir)

os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号
loaded_model = models.resnet152()   #注意这里需要对模型结构有定义
loaded_model.load_state_dict(torch.load(save_dir))
loaded_model = nn.DataParallel(loaded_model).cuda()
loaded_model.state_dict = loaded_dict
```

単一 GPU でも `DataParallel` でラップすれば学習可能です。

**キーから `module.` を除去する**

```python
from collections import OrderedDict
os.environ['CUDA_VISIBLE_DEVICES'] = '0'   #这里替换成希望使用的GPU编号

loaded_dict = torch.load(save_dir)

new_state_dict = OrderedDict()
for k, v in loaded_dict.items():
    name = k[7:] # 先頭の 'module.' を除去
    new_state_dict[name] = v #新字典的key值对应的value一一对应

loaded_model = models.resnet152()   # 構造定義が必要
loaded_model.state_dict = new_state_dict
loaded_model = loaded_model.cuda()
```

**使用replace操作去除module**

```python
loaded_model = models.resnet152()    
loaded_dict = torch.load(save_dir)
loaded_model.load_state_dict({k.replace('module.', ''): v for k, v in loaded_dict.items()})
```



- **多卡保存+多卡加载**

由于是模型保存和加载都使用的是多卡，因此不存在模型层名前缀不同的问题。但多卡状态下存在一个device（使用的GPU）匹配的问题，即**保存整个模型**时会同时保存所使用的GPU id等信息，读取时若这些信息和当前使用的GPU信息不符则可能会报错或者程序不按预定状态运行。具体表现为以下两点：

**读取整个模型再使用nn.DataParallel进行分布式训练设置**

这种情况很可能会造成保存的整个模型中GPU id和读取环境下设置的GPU id不符，训练时数据所在device和模型所在device不一致而报错。

**读取整个模型而不使用nn.DataParallel进行分布式训练设置**

这种情况可能不会报错，测试中发现程序会自动使用设备的前n个GPU进行训练（n是保存的模型使用的GPU个数）。此时如果指定的GPU个数少于n，则会报错。在这种情况下，只有保存模型时环境的device id和读取模型时环境的device id一致，程序才会按照预期在指定的GPU上进行分布式训练。

相比之下，读取模型权重，之后再使用nn.DataParallel进行分布式训练设置则没有问题。因此**多卡模式下建议使用权重的方式存储和读取模型**：

```python
import os
import torch
from torchvision import models

os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'   #这里替换成希望使用的GPU编号

model = models.resnet152(pretrained=True)
model = nn.DataParallel(model).cuda()

# 保存+读取模型权重，强烈建议！！
torch.save(model.state_dict(), save_dir)
loaded_model = models.resnet152()   #注意这里需要对模型结构有定义
loaded_model.load_state_dict(torch.load(save_dir)))
loaded_model = nn.DataParallel(loaded_model).cuda()
```

如果只有保存的整个模型，也可以采用提取权重的方式构建新的模型：

```python
# 读取整个模型
loaded_whole_model = torch.load(save_dir)
loaded_model = models.resnet152()   #注意这里需要对模型结构有定义
loaded_model.state_dict = loaded_whole_model.state_dict
loaded_model = nn.DataParallel(loaded_model).cuda()
```

另外，上面所有对于loaded_model修改权重字典的形式都是通过赋值来实现的，在PyTorch中还可以通过"load_state_dict"函数来实现。因此在上面的所有示例中，我们使用了两种实现方式。

```python
loaded_model.load_state_dict(loaded_dict)
```

## 5.4.5 其他参数的保存和读取
在深度学习项目里，有时候我们不仅仅需要保存模型的权重，还需要保存一些其他的参数，比如训练的epoch数、训练的loss，优化器的参数，动态调整学习策略的参数等等。这些参数可以通过字典的形式保存在一个文件里，然后在读取模型时一起读取。这里我们以下方代码为例：
```python
torch.save({
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'lr_scheduler': lr_scheduler.state_dict(),
        'epoch': epoch,
        'args': args,
    }, checkpoint_path)
```
这些参数的读取方式也是类似的：
```python
checkpoint = torch.load(checkpoint_path)
model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer'])
lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])
epoch = checkpoint['epoch']
args = checkpoint['args']
```

## 附：测试环境

OS: Ubuntu 20.04 LTS GPU: GeForce RTX 2080 Ti (x3)



## 参考资料

本章内容同时发布于[知乎](https://zhuanlan.zhihu.com/p/371090724)和[CSDN](https://blog.csdn.net/goodljq/article/details/117258032)

1. [pytorch 中pkl和pth的区别？](https://www.zhihu.com/question/274533811)  
2. [What is the difference between .pt, .pth and .pwf extentions in PyTorch?](https://stackoverflow.com/questions/59095824/what-is-the-difference-between-pt-pth-and-pwf-extentions-in-pytorch)
